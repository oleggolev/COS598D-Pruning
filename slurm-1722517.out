Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.46it/s]100%|██████████| 1/1 [00:00<00:00,  3.45it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.26s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.28s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302601          10.00          49.95
Final      10      0.5212   0.553549          80.86          98.83
Prune results:
             module   param  ...  score abs sum  prunable
0    layers.0.conv  weight  ...   2.945027e+22      True
1    layers.0.conv    bias  ...   0.000000e+00     False
2    layers.1.conv  weight  ...   2.945027e+22      True
3    layers.1.conv    bias  ...   0.000000e+00     False
4    layers.3.conv  weight  ...   2.945027e+22      True
5    layers.3.conv    bias  ...   0.000000e+00     False
6    layers.4.conv  weight  ...   2.945027e+22      True
7    layers.4.conv    bias  ...   0.000000e+00     False
8    layers.6.conv  weight  ...   2.945027e+22      True
9    layers.6.conv    bias  ...   0.000000e+00     False
10   layers.7.conv  weight  ...   2.945027e+22      True
11   layers.7.conv    bias  ...   0.000000e+00     False
12   layers.8.conv  weight  ...   2.945027e+22      True
13   layers.8.conv    bias  ...   0.000000e+00     False
14  layers.10.conv  weight  ...   2.945027e+22      True
15  layers.10.conv    bias  ...   0.000000e+00     False
16  layers.11.conv  weight  ...   2.945026e+22      True
17  layers.11.conv    bias  ...   0.000000e+00     False
18  layers.12.conv  weight  ...   2.945027e+22      True
19  layers.12.conv    bias  ...   0.000000e+00     False
20  layers.14.conv  weight  ...   2.945026e+22      True
21  layers.14.conv    bias  ...   0.000000e+00     False
22  layers.15.conv  weight  ...   2.945026e+22      True
23  layers.15.conv    bias  ...   0.000000e+00     False
24  layers.16.conv  weight  ...   2.945028e+22      True
25  layers.16.conv    bias  ...   0.000000e+00     False
26              fc  weight  ...   2.945027e+22      True
27              fc    bias  ...   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 143301021/313478154 (0.4571)
Time:  93.62823783606291
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]100%|██████████| 1/1 [00:00<00:00,  5.91it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.25s/it] 20%|██        | 2/10 [00:18<01:14,  9.26s/it] 30%|███       | 3/10 [00:27<01:04,  9.27s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.25s/it] 60%|██████    | 6/10 [00:55<00:37,  9.27s/it]