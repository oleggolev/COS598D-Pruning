Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]100%|██████████| 1/1 [00:00<00:00,  5.91it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:24,  9.35s/it] 20%|██        | 2/10 [00:18<01:14,  9.32s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.32s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.26s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302585          10.04          49.65
Final      10    2.302675   2.302590          10.00          50.00
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.100450    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.099881    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.099569   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.099575   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.100744   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.099660   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.100046  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.099786  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.099997  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.100182  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.100006  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.099996  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.093555     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 31597941/313478154 (0.1008)
Time:  93.62897612527013
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.24it/s]100%|██████████| 1/1 [00:00<00:00,  6.24it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:05,  9.29s/it] 40%|████      | 4/10 [00:37<00:55,  9.31s/it] 50%|█████     | 5/10 [00:46<00:46,  9.30s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.303128          11.55          49.51
Final      10    0.574813   0.604931          79.07          98.64
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.896412     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.516276    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.516859    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.361070   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.360372   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.194650   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.194641   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.195350  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.067016  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.066820  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.067069  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.067190  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.066990  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.533008     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 76875004/313478154 (0.2452)
Time:  93.74832649249583
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.88it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.26s/it] 30%|███       | 3/10 [00:27<01:04,  9.27s/it] 40%|████      | 4/10 [00:37<00:55,  9.28s/it] 50%|█████     | 5/10 [00:46<00:46,  9.31s/it] 60%|██████    | 6/10 [00:55<00:37,  9.30s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.31s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.321671           9.71          50.37
Final      10    0.645541   0.689909          75.71          98.19
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.861111     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.520698    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.381497    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.252387   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.207723   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.135123   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.146440   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.130804  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.082429  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.082551  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.099387  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.079112  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.080396  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.751758     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 63876719/313478154 (0.2038)
Time:  93.69600168429315
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.23it/s]100%|██████████| 1/1 [00:00<00:00,  1.23it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.31s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  7.878097e+09          10.00          50.00
Final      10  3177.891179  3.803166e+03          10.98          53.76
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.538194     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.370334    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.307210    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.223083   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.190141   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.143265   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.147013   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.124938  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.081387  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.085459  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.087292  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.085674  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.094243  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.526953     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450944e-05    8.899751e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294465e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038072e-06    1.728672e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169521e-07    5.200793e-10   0.076228    7.789811e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561255e-07    2.033139e-10   0.075534    4.886780e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130130e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226843e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004287e-08    1.233988e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533983e-12   0.066115    7.976353e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921838e-12   0.066509    7.930851e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176804e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784301e-08    3.842034e-12   0.065690    7.952882e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729265e-08    4.991517e-12   0.064391    9.028376e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451926e-05    4.614434e-09   0.074339    3.635964e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731965e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663232e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476658e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596654e-10       1.148654      True  
7         0.000000e+00       0.000000     False  
8         1.794989e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525789e-11       1.451823      True  
11        0.000000e+00       0.000000     False  
12        2.785695e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681743      True  
15        0.000000e+00       0.000000     False  
16        2.898547e-12       1.881858      True  
17        0.000000e+00       0.000000     False  
18        2.293649e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548664e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210326e-12       1.876320      True  
23        0.000000e+00       0.000000     False  
24        4.177146e-12       2.130061      True  
25        0.000000e+00       0.000000     False  
26        3.503219e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 55155144/313478154 (0.1759)
Time:  93.715984987095
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]100%|██████████| 1/1 [00:00<00:00,  3.04it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.32s/it] 20%|██        | 2/10 [00:18<01:14,  9.31s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.31s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.30s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302601          10.00          49.95
Final      10    0.510033   0.585219          79.31          98.80
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.484929   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.484745   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.994922     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 143301021/313478154 (0.4571)
Time:  93.72890188917518
Saving results.
Loading mnist dataset.
Creating default-fc model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 459.75it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:02<00:21,  2.44s/it] 20%|██        | 2/10 [00:04<00:19,  2.44s/it] 30%|███       | 3/10 [00:07<00:17,  2.43s/it] 40%|████      | 4/10 [00:09<00:14,  2.42s/it] 50%|█████     | 5/10 [00:12<00:12,  2.41s/it] 60%|██████    | 6/10 [00:14<00:09,  2.40s/it] 70%|███████   | 7/10 [00:16<00:07,  2.39s/it] 80%|████████  | 8/10 [00:19<00:04,  2.39s/it] 90%|█████████ | 9/10 [00:21<00:02,  2.41s/it]100%|██████████| 10/10 [00:24<00:00,  2.40s/it]100%|██████████| 10/10 [00:24<00:00,  2.41s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.306855          10.32          47.39
Pre-Prune  0          NaN   2.306855          10.32          47.39
Post-Prune 0          NaN   2.305041          10.32          49.26
Final      10    0.193536   0.195471          94.11          99.73
Prune results:
    module   param  sparsity   size       shape  flops  score mean  \
0       1  weight  0.100587  78400  (100, 784)  78400    0.002703   
1       1    bias  1.000000    100      (100,)    100    0.000000   
2       3  weight  0.098500  10000  (100, 100)  10000   -0.004195   
3       3    bias  1.000000    100      (100,)    100    0.000000   
4       5  weight  0.100200  10000  (100, 100)  10000    0.010099   
5       5    bias  1.000000    100      (100,)    100    0.000000   
6       7  weight  0.098300  10000  (100, 100)  10000   -0.002534   
7       7    bias  1.000000    100      (100,)    100    0.000000   
8       9  weight  0.097900  10000  (100, 100)  10000   -0.012221   
9       9    bias  1.000000    100      (100,)    100    0.000000   
10     11  weight  0.105000   1000   (10, 100)   1000   -0.024213   
11     11    bias  1.000000     10       (10,)     10    0.000000   

    score variance   score sum  score abs mean  score abs variance  \
0         1.000494  211.893555        0.797696            0.364183   
1         0.000000    0.000000        0.000000            0.000000   
2         0.995103  -41.945179        0.792052            0.367775   
3         0.000000    0.000000        0.000000            0.000000   
4         0.975469  100.989647        0.789619            0.352073   
5         0.000000    0.000000        0.000000            0.000000   
6         0.988294  -25.344933        0.791694            0.361521   
7         0.000000    0.000000        0.000000            0.000000   
8         0.982350 -122.213615        0.792565            0.354341   
9         0.000000    0.000000        0.000000            0.000000   
10        1.035793  -24.212658        0.826184            0.353799   
11        0.000000    0.000000        0.000000            0.000000   

    score abs sum  prunable  
0    62539.339844      True  
1        0.000000     False  
2     7920.516602      True  
3        0.000000     False  
4     7896.190430      True  
5        0.000000     False  
6     7916.939453      True  
7        0.000000     False  
8     7925.648438      True  
9        0.000000     False  
10     826.184204      True  
11       0.000000     False  
Parameter Sparsity: 12450/119910 (0.1038)
FLOP Sparsity: 12450/119910 (0.1038)
Time:  24.551289721392095
Saving results.
Loading mnist dataset.
Creating default-fc model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 429.61it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:02<00:21,  2.39s/it] 20%|██        | 2/10 [00:04<00:19,  2.39s/it] 30%|███       | 3/10 [00:07<00:16,  2.41s/it] 40%|████      | 4/10 [00:09<00:14,  2.41s/it] 50%|█████     | 5/10 [00:12<00:12,  2.42s/it] 60%|██████    | 6/10 [00:14<00:09,  2.43s/it] 70%|███████   | 7/10 [00:16<00:07,  2.41s/it] 80%|████████  | 8/10 [00:19<00:04,  2.40s/it] 90%|█████████ | 9/10 [00:21<00:02,  2.40s/it]100%|██████████| 10/10 [00:24<00:00,  2.40s/it]100%|██████████| 10/10 [00:24<00:00,  2.41s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.306855          10.32          47.39
Pre-Prune  0          NaN   2.306855          10.32          47.39
Post-Prune 0          NaN   2.306222          10.32          49.80
Final      10    2.301311   2.301112          11.35          51.63
Prune results:
    module   param  sparsity   size       shape  flops  score mean  \
0       1  weight    0.0000  78400  (100, 784)  78400    0.017801   
1       1    bias    1.0000    100      (100,)    100    0.000000   
2       3  weight    0.2911  10000  (100, 100)  10000    0.049968   
3       3    bias    1.0000    100      (100,)    100    0.000000   
4       5  weight    0.2895  10000  (100, 100)  10000    0.049833   
5       5    bias    1.0000    100      (100,)    100    0.000000   
6       7  weight    0.2872  10000  (100, 100)  10000    0.049674   
7       7    bias    1.0000    100      (100,)    100    0.000000   
8       9  weight    0.2974  10000  (100, 100)  10000    0.050118   
9       9    bias    1.0000    100      (100,)    100    0.000000   
10     11  weight    0.2880   1000   (10, 100)   1000    0.048696   
11     11    bias    1.0000     10       (10,)     10    0.000000   

    score variance    score sum  score abs mean  score abs variance  \
0         0.000107  1395.579224        0.017801            0.000107   
1         0.000000     0.000000        0.000000            0.000000   
2         0.000839   499.683289        0.049968            0.000839   
3         0.000000     0.000000        0.000000            0.000000   
4         0.000824   498.329956        0.049833            0.000824   
5         0.000000     0.000000        0.000000            0.000000   
6         0.000828   496.741730        0.049674            0.000828   
7         0.000000     0.000000        0.000000            0.000000   
8         0.000836   501.181396        0.050118            0.000836   
9         0.000000     0.000000        0.000000            0.000000   
10        0.000822    48.696388        0.048696            0.000822   
11        0.000000     0.000000        0.000000            0.000000   

    score abs sum  prunable  
0     1395.579224      True  
1        0.000000     False  
2      499.683289      True  
3        0.000000     False  
4      498.329956      True  
5        0.000000     False  
6      496.741730      True  
7        0.000000     False  
8      501.181396      True  
9        0.000000     False  
10      48.696388      True  
11       0.000000     False  
Parameter Sparsity: 12449/119910 (0.1038)
FLOP Sparsity: 12449/119910 (0.1038)
Time:  24.487352413125336
Saving results.
Loading mnist dataset.
Creating default-fc model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.02it/s]100%|██████████| 1/1 [00:00<00:00,  6.01it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:02<00:22,  2.46s/it] 20%|██        | 2/10 [00:04<00:19,  2.44s/it] 30%|███       | 3/10 [00:07<00:16,  2.41s/it] 40%|████      | 4/10 [00:09<00:14,  2.38s/it] 50%|█████     | 5/10 [00:11<00:11,  2.38s/it] 60%|██████    | 6/10 [00:14<00:09,  2.38s/it] 70%|███████   | 7/10 [00:16<00:07,  2.39s/it] 80%|████████  | 8/10 [00:19<00:04,  2.40s/it] 90%|█████████ | 9/10 [00:21<00:02,  2.39s/it]100%|██████████| 10/10 [00:23<00:00,  2.38s/it]100%|██████████| 10/10 [00:23<00:00,  2.39s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.306855          10.32          47.39
Pre-Prune  0          NaN   2.306855          10.32          47.39
Post-Prune 0          NaN   2.306854          10.32          49.80
Final      10    0.129924   0.137349          95.67          99.82
Prune results:
    module   param  sparsity   size       shape  flops  score mean  \
0       1  weight  0.054056  78400  (100, 784)  78400    0.000005   
1       1    bias  1.000000    100      (100,)    100    0.000000   
2       3  weight  0.218900  10000  (100, 100)  10000    0.000013   
3       3    bias  1.000000    100      (100,)    100    0.000000   
4       5  weight  0.181800  10000  (100, 100)  10000    0.000011   
5       5    bias  1.000000    100      (100,)    100    0.000000   
6       7  weight  0.151100  10000  (100, 100)  10000    0.000010   
7       7    bias  1.000000    100      (100,)    100    0.000000   
8       9  weight  0.176600  10000  (100, 100)  10000    0.000016   
9       9    bias  1.000000    100      (100,)    100    0.000000   
10     11  weight  0.418000   1000   (10, 100)   1000    0.000096   
11     11    bias  1.000000     10       (10,)     10    0.000000   

    score variance  score sum  score abs mean  score abs variance  \
0     5.989315e-11   0.404448        0.000005        5.989315e-11   
1     0.000000e+00   0.000000        0.000000        0.000000e+00   
2     4.284688e-10   0.125934        0.000013        4.284688e-10   
3     0.000000e+00   0.000000        0.000000        0.000000e+00   
4     4.830765e-10   0.111812        0.000011        4.830765e-10   
5     0.000000e+00   0.000000        0.000000        0.000000e+00   
6     6.846700e-10   0.103630        0.000010        6.846700e-10   
7     0.000000e+00   0.000000        0.000000        0.000000e+00   
8     2.011727e-09   0.158670        0.000016        2.011727e-09   
9     0.000000e+00   0.000000        0.000000        0.000000e+00   
10    5.414625e-08   0.095505        0.000096        5.414625e-08   
11    0.000000e+00   0.000000        0.000000        0.000000e+00   

    score abs sum  prunable  
0        0.404448      True  
1        0.000000     False  
2        0.125934      True  
3        0.000000     False  
4        0.111812      True  
5        0.000000     False  
6        0.103630      True  
7        0.000000     False  
8        0.158670      True  
9        0.000000     False  
10       0.095505      True  
11       0.000000     False  
Parameter Sparsity: 12449/119910 (0.1038)
FLOP Sparsity: 12449/119910 (0.1038)
Time:  24.343755647540092
Saving results.
Loading mnist dataset.
Creating default-fc model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.16it/s]100%|██████████| 1/1 [00:00<00:00,  3.16it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:02<00:21,  2.40s/it] 20%|██        | 2/10 [00:04<00:19,  2.42s/it] 30%|███       | 3/10 [00:07<00:16,  2.41s/it] 40%|████      | 4/10 [00:09<00:14,  2.40s/it] 50%|█████     | 5/10 [00:12<00:12,  2.40s/it] 60%|██████    | 6/10 [00:14<00:09,  2.42s/it] 70%|███████   | 7/10 [00:16<00:07,  2.43s/it] 80%|████████  | 8/10 [00:19<00:04,  2.42s/it] 90%|█████████ | 9/10 [00:21<00:02,  2.42s/it]100%|██████████| 10/10 [00:24<00:00,  2.41s/it]100%|██████████| 10/10 [00:24<00:00,  2.41s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.306855          10.32          47.39
Pre-Prune  0          NaN   2.306855          10.32          47.39
Post-Prune 0          NaN   2.387675          10.08          49.31
Final      10    0.163503   0.167285          95.09          99.75
Prune results:
    module   param  sparsity   size       shape  flops    score mean  \
0       1  weight  0.051645  78400  (100, 784)  78400  8.022544e-07   
1       1    bias  1.000000    100      (100,)    100  0.000000e+00   
2       3  weight  0.216500  10000  (100, 100)  10000  1.698872e-05   
3       3    bias  1.000000    100      (100,)    100  0.000000e+00   
4       5  weight  0.206200  10000  (100, 100)  10000  1.701438e-05   
5       5    bias  1.000000    100      (100,)    100  0.000000e+00   
6       7  weight  0.176400  10000  (100, 100)  10000  1.790806e-05   
7       7    bias  1.000000    100      (100,)    100  0.000000e+00   
8       9  weight  0.156500  10000  (100, 100)  10000  2.057313e-05   
9       9    bias  1.000000    100      (100,)    100  0.000000e+00   
10     11  weight  0.335000   1000   (10, 100)   1000  2.119170e-04   
11     11    bias  1.000000     10       (10,)     10  0.000000e+00   

    score variance  score sum  score abs mean  score abs variance  \
0     5.569648e-10   0.062897        0.000013        4.010847e-10   
1     0.000000e+00   0.000000        0.000000        0.000000e+00   
2     9.410978e-09   0.169887        0.000045        7.668780e-09   
3     0.000000e+00   0.000000        0.000000        0.000000e+00   
4     1.349678e-08   0.170144        0.000052        1.109023e-08   
5     0.000000e+00   0.000000        0.000000        0.000000e+00   
6     2.519507e-08   0.179081        0.000061        2.182158e-08   
7     0.000000e+00   0.000000        0.000000        0.000000e+00   
8     3.435935e-08   0.205731        0.000066        3.037865e-08   
9     0.000000e+00   0.000000        0.000000        0.000000e+00   
10    4.173014e-07   0.211917        0.000315        3.629367e-07   
11    0.000000e+00   0.000000        0.000000        0.000000e+00   

    score abs sum  prunable  
0        0.980858      True  
1        0.000000     False  
2        0.450646      True  
3        0.000000     False  
4        0.519234      True  
5        0.000000     False  
6        0.607798      True  
7        0.000000     False  
8        0.663623      True  
9        0.000000     False  
10       0.315077      True  
11       0.000000     False  
Parameter Sparsity: 12450/119910 (0.1038)
FLOP Sparsity: 12450/119910 (0.1038)
Time:  24.57600427698344
Saving results.
Loading mnist dataset.
Creating default-fc model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.58it/s]100%|██████████| 1/1 [00:00<00:00,  6.57it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:02<00:21,  2.38s/it] 20%|██        | 2/10 [00:04<00:19,  2.39s/it] 30%|███       | 3/10 [00:07<00:16,  2.38s/it] 40%|████      | 4/10 [00:09<00:14,  2.38s/it] 50%|█████     | 5/10 [00:11<00:11,  2.38s/it] 60%|██████    | 6/10 [00:14<00:09,  2.39s/it] 70%|███████   | 7/10 [00:16<00:07,  2.39s/it] 80%|████████  | 8/10 [00:19<00:04,  2.39s/it] 90%|█████████ | 9/10 [00:21<00:02,  2.39s/it]100%|██████████| 10/10 [00:23<00:00,  2.38s/it]100%|██████████| 10/10 [00:23<00:00,  2.39s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.306855          10.32          47.39
Pre-Prune  0          NaN   2.306855          10.32          47.39
Post-Prune 0          NaN   2.306644          10.32          49.80
Final      10    2.301366   2.301025          11.35          52.14
Prune results:
    module   param  sparsity   size       shape  flops  score mean  \
0       1  weight    0.0000  78400  (100, 784)  78400    5.375414   
1       1    bias    1.0000    100      (100,)    100    0.000000   
2       3  weight    0.2801  10000  (100, 100)  10000   42.197372   
3       3    bias    1.0000    100      (100,)    100    0.000000   
4       5  weight    0.2774  10000  (100, 100)  10000   42.228401   
5       5    bias    1.0000    100      (100,)    100    0.000000   
6       7  weight    0.2796  10000  (100, 100)  10000   42.234432   
7       7    bias    1.0000    100      (100,)    100    0.000000   
8       9  weight    0.2638  10000  (100, 100)  10000   42.235725   
9       9    bias    1.0000    100      (100,)    100    0.000000   
10     11  weight    0.9310   1000   (10, 100)   1000  422.359253   
11     11    bias    1.0000     10       (10,)     10    0.000000   

    score variance     score sum  score abs mean  score abs variance  \
0         9.872968  421432.43750        5.375414            9.872968   
1         0.000000       0.00000        0.000000            0.000000   
2       606.436279  421973.71875       42.197372          606.436279   
3         0.000000       0.00000        0.000000            0.000000   
4       605.871521  422284.00000       42.228401          605.871521   
5         0.000000       0.00000        0.000000            0.000000   
6       616.338745  422344.31250       42.234432          616.338745   
7         0.000000       0.00000        0.000000            0.000000   
8       690.546692  422357.25000       42.235725          690.546692   
9         0.000000       0.00000        0.000000            0.000000   
10    62346.089844  422359.25000      422.359253        62346.089844   
11        0.000000       0.00000        0.000000            0.000000   

    score abs sum  prunable  
0    421432.43750      True  
1         0.00000     False  
2    421973.71875      True  
3         0.00000     False  
4    422284.00000      True  
5         0.00000     False  
6    422344.31250      True  
7         0.00000     False  
8    422357.25000      True  
9         0.00000     False  
10   422359.25000      True  
11        0.00000     False  
Parameter Sparsity: 12449/119910 (0.1038)
FLOP Sparsity: 12449/119910 (0.1038)
Time:  24.299352685920894
Saving results.
Experiment 'singleshot' with expid 'hp-rand-0.05' exists.  Overwrite (yes/no)? Traceback (most recent call last):
  File "main.py", line 120, in <module>
    os.makedirs(result_dir)
  File "/usr/lib64/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/scratch/network/vikashm/Results/data_sparsity_order2/singleshot/hp-rand-0.05'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 124, in <module>
    val = input("Experiment '{}' with expid '{}' exists.  Overwrite (yes/no)? ".format(args.experiment, args.expid))
EOFError: EOF when reading a line
Experiment 'singleshot' with expid 'hp-mag-0.05' exists.  Overwrite (yes/no)? Traceback (most recent call last):
  File "main.py", line 120, in <module>
    os.makedirs(result_dir)
  File "/usr/lib64/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/scratch/network/vikashm/Results/data_sparsity_order2/singleshot/hp-mag-0.05'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main.py", line 124, in <module>
    val = input("Experiment '{}' with expid '{}' exists.  Overwrite (yes/no)? ".format(args.experiment, args.expid))
EOFError: EOF when reading a line
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  2.89it/s]100%|██████████| 1/1 [00:00<00:00,  2.88it/s]
ERROR: 12249879.0 prunable parameters remaining, expected 13115278.045185937
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.17it/s]100%|██████████| 1/1 [00:00<00:00,  1.17it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.25s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.30s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]
Train results:
                train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN  2.417717e+00          11.73          50.17
Pre-Prune  0          NaN  2.417717e+00          11.73          50.17
Post-Prune 0          NaN  1.027184e+10          10.00          50.00
Final      10    2.023582  1.994069e+00          27.40          79.54
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.584491     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.625434    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.692925    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.774000   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.802568   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.849911   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.845562   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.865981  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.909701  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.904541  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.903065  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.906094  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.896824  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.620117     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450945e-05    8.899752e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294466e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038073e-06    1.728673e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169524e-07    5.200794e-10   0.076228    7.789813e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561255e-07    2.033140e-10   0.075534    4.886781e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130131e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226842e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004286e-08    1.233988e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533982e-12   0.066115    7.976350e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921837e-12   0.066509    7.930847e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176802e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784301e-08    3.842033e-12   0.065690    7.952879e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729263e-08    4.991516e-12   0.064391    9.028375e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451925e-05    4.614429e-09   0.074339    3.635964e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731965e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663232e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476659e-09       1.172929      True  
5         0.000000e+00       0.000000     False  
6         4.596655e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794989e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525791e-11       1.451824      True  
11        0.000000e+00       0.000000     False  
12        2.785695e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898545e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293649e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548662e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210324e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177145e-12       2.130061      True  
25        0.000000e+00       0.000000     False  
26        3.503215e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 13119513/14719818 (0.8913)
FLOP Sparsity: 256736314/313478154 (0.8190)
Time:  93.65553299617022
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]100%|██████████| 1/1 [00:00<00:00,  3.15it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.27s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:04<00:27,  9.28s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.27s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.414332          11.66          50.38
Final      10    0.594168   0.661222          78.22          98.23
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.998264    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.996460    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.993198   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.986152   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.972612   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.972772   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.944177  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.888635  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.888531  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.865655  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.866078  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.856663  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.999609     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 13119512/14719818 (0.8913)
FLOP Sparsity: 297416253/313478154 (0.9488)
Time:  93.42781428433955
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.74it/s]100%|██████████| 1/1 [00:00<00:00,  5.74it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.26s/it] 30%|███       | 3/10 [00:27<01:04,  9.26s/it] 40%|████      | 4/10 [00:37<00:55,  9.26s/it] 50%|█████     | 5/10 [00:46<00:46,  9.26s/it] 60%|██████    | 6/10 [00:55<00:37,  9.25s/it] 70%|███████   | 7/10 [01:04<00:27,  9.26s/it] 80%|████████  | 8/10 [01:14<00:18,  9.25s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.26s/it]100%|██████████| 10/10 [01:32<00:00,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.26s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.307499          10.13          48.79
Final      10    0.589395   0.660295          78.29          97.96
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.780671     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.795193    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.794135    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.794040   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.794413   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.793855   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.795035   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.794455  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.794583  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.793937  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.794324  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.794401  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.794298  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.787305     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 249063429/313478154 (0.7945)
Time:  93.31040227785707
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.94it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.27s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.28s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:04<00:27,  9.28s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.413360           9.73          50.08
Final      10    0.605624   0.617132          79.19          98.40
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.983796     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.920085    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.919515    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.887573   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.888631   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.843421   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.843079   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.843273  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.780084  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.779522  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.779595  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.779892  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.779687  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.931055     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 11693236/14719818 (0.7944)
FLOP Sparsity: 264914296/313478154 (0.8451)
Time:  93.52833960670978
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.38it/s]100%|██████████| 1/1 [00:00<00:00,  3.37it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.26s/it] 30%|███       | 3/10 [00:27<01:04,  9.27s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.27s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:04<00:27,  9.28s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.425654          12.02          50.27
Final      10    0.590449   0.645671          78.72          98.07
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.999485    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.998047    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.992038   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.982849   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.963759   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.958817   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.936362  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.894237  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.888953  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.756611  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.614114  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.617171  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.985547     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 290937951/313478154 (0.9281)
Time:  93.52875826694071
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.19it/s]100%|██████████| 1/1 [00:00<00:00,  1.19it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.31s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN  2.417717e+00          11.73          50.17
Pre-Prune  0          NaN  2.417717e+00          11.73          50.17
Post-Prune 0          NaN  2.631110e+10          10.00          50.00
Final      10    2.113354  2.052319e+00          25.76          75.38
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.571759     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.562636    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.619941    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.690850   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.717431   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.764628   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.747426   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.760220  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.806300  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.789942  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.799729  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.823098  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.814515  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.598242     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450945e-05    8.899752e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294467e-09   0.076499    2.520992e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038073e-06    1.728673e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169524e-07    5.200795e-10   0.076228    7.789815e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561255e-07    2.033140e-10   0.075534    4.886782e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130132e-11   0.073053    2.461453e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226843e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004284e-08    1.233988e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533982e-12   0.066115    7.976353e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921837e-12   0.066509    7.930850e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725551e-08    3.176804e-12   0.064304    7.930211e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784302e-08    3.842035e-12   0.065690    7.952880e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729264e-08    4.991515e-12   0.064391    9.028374e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451925e-05    4.614435e-09   0.074339    3.635964e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731968e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663233e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476659e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596655e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794990e-10       1.441171      True  
9         0.000000e+00       0.000000     False  
10        4.525791e-11       1.451824      True  
11        0.000000e+00       0.000000     False  
12        2.785696e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681743      True  
15        0.000000e+00       0.000000     False  
16        2.898546e-12       1.881858      True  
17        0.000000e+00       0.000000     False  
18        2.293649e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548664e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210325e-12       1.876320      True  
23        0.000000e+00       0.000000     False  
24        4.177145e-12       2.130061      True  
25        0.000000e+00       0.000000     False  
26        3.503220e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 228535402/313478154 (0.7290)
Time:  93.61716663930565
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.17it/s]100%|██████████| 1/1 [00:00<00:00,  3.17it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.28s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.31s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.403514          11.46          49.92
Final      10    0.572867   0.602694          79.39          98.49
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.999421     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.996582    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.993096    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.986925   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.974199   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.947367   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.947562   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.892713  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.788353  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.787956  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.746524  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.746590  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.731970  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.999609     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 282939167/313478154 (0.9026)
Time:  93.70661922544241
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.32s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.27s/it] 70%|███████   | 7/10 [01:04<00:27,  9.27s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302965          10.01          49.83
Final      10    0.546992   0.602483          80.22          98.19
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.615162     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.631673    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.632053    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.629747   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.630354   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.629730   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.631607   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.630873  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.631227  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.630902  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.630989  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.630869  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.631136  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.616016     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 197841838/313478154 (0.6311)
Time:  93.51443935930729
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.422262          10.01          49.55
Final      10    0.565053   0.618626          79.29          98.44
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.969329     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.855089    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.853841    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.795681   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.795546   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.715176   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.715395   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.715451  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.605924  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.605485  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.605685  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.605615  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.605574  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.865820     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 225822983/313478154 (0.7204)
Time:  93.62290543410927
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]100%|██████████| 1/1 [00:00<00:00,  3.02it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.27s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:04<00:27,  9.28s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.438170          10.36          51.83
Final      10     0.56189   0.622504          79.46          98.44
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.995370     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.970486    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.935208    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.872165   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.841359   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.769906   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.793235   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.775041  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.677802  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.694550  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.613603  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.484218  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.480073  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.970312     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 245462513/313478154 (0.7830)
Time:  93.55631818715483
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.31s/it] 20%|██        | 2/10 [00:18<01:14,  9.33s/it] 30%|███       | 3/10 [00:27<01:05,  9.32s/it] 40%|████      | 4/10 [00:37<00:56,  9.33s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
Train results:
                train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN  2.417717e+00          11.73          50.17
Pre-Prune  0          NaN  2.417717e+00          11.73          50.17
Post-Prune 0          NaN  3.716465e+10          10.00          50.00
Final      10    2.143609  2.083248e+00          24.52          73.42
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.563079     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.507704    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.524482    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.546441   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.553433   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.576696   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.563794   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.571639  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.595076  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.588372  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.642397  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.705760  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.703569  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.586719     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450945e-05    8.899755e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294467e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038073e-06    1.728673e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169525e-07    5.200795e-10   0.076228    7.789813e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561256e-07    2.033140e-10   0.075535    4.886782e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130132e-11   0.073053    2.461453e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226844e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004284e-08    1.233988e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533981e-12   0.066115    7.976350e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921836e-12   0.066509    7.930848e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176802e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784302e-08    3.842033e-12   0.065690    7.952879e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729265e-08    4.991513e-12   0.064391    9.028373e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451925e-05    4.614427e-09   0.074339    3.635963e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731968e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663233e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476659e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596655e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794989e-10       1.441171      True  
9         0.000000e+00       0.000000     False  
10        4.525791e-11       1.451824      True  
11        0.000000e+00       0.000000     False  
12        2.785696e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898545e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293648e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548663e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210324e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177143e-12       2.130060      True  
25        0.000000e+00       0.000000     False  
26        3.503213e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 9289138/14719818 (0.6311)
FLOP Sparsity: 179290970/313478154 (0.5719)
Time:  93.76109366305172
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]100%|██████████| 1/1 [00:00<00:00,  3.09it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.29s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.28s/it] 60%|██████    | 6/10 [00:55<00:37,  9.27s/it] 70%|███████   | 7/10 [01:04<00:27,  9.27s/it] 80%|████████  | 8/10 [01:14<00:18,  9.27s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.396131          10.05          51.06
Final      10    0.576887   0.628854          78.86          98.25
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.998843     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.993490    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.987115    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.974752   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.950578   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.799322  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.611696  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.611219  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.549993  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.550156  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.533391  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.999219     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 257583398/313478154 (0.8217)
Time:  93.53779249358922
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.02it/s]100%|██████████| 1/1 [00:00<00:00,  6.02it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302582           9.66          50.48
Final      10    0.623199   0.649949          77.75          98.02
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.306713     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.314372    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.317532    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.313775   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.315433   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.316316   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.315694   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.316327  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.316288  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.316055  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.316956  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.315884  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.316285  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.306836     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 99134651/313478154 (0.3162)
Time:  93.62936366070062
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.26s/it] 20%|██        | 2/10 [00:18<01:14,  9.26s/it] 30%|███       | 3/10 [00:27<01:04,  9.27s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.27s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:04<00:27,  9.28s/it] 80%|████████  | 8/10 [01:14<00:18,  9.27s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.329496           8.82          49.56
Final      10    0.524494   0.579123          80.44          98.81
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.938657     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.701606    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.699517    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.588182   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.587646   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.442337   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.441876   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.442021  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.276644  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.276784  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.277225  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.277301  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.277092  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.720117     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 145383527/313478154 (0.4638)
Time:  93.47490031924099
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.27s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.28s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.441820           8.77          50.62
Final      10    0.570698   0.611014          78.96          98.68
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.961227     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.810818    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.696221    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.557285   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.497189   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.393904   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.428480   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.405860  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.308576  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.317776  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.319197  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.241647  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.242628  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.894727     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 4657709/14719818 (0.3164)
FLOP Sparsity: 146081773/313478154 (0.4660)
Time:  93.63966094702482
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.32s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN  2.417717e+00          11.73          50.17
Pre-Prune  0          NaN  2.417717e+00          11.73          50.17
Post-Prune 0          NaN  4.057930e+10          10.00          50.00
Final      10    2.041814  1.939509e+00          29.47          80.64
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.560185     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.483209    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.455471    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.407023   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.384281   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.349433   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.371523   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.364052  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.328551  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.346779  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.311499  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.260489  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.265916  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.568750     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450944e-05    8.899751e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294464e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038072e-06    1.728672e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169522e-07    5.200793e-10   0.076228    7.789812e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561254e-07    2.033139e-10   0.075534    4.886781e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130131e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226842e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004282e-08    1.233988e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533981e-12   0.066115    7.976350e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921837e-12   0.066509    7.930847e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176803e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784302e-08    3.842033e-12   0.065690    7.952879e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729265e-08    4.991512e-12   0.064391    9.028372e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451925e-05    4.614427e-09   0.074339    3.635963e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731965e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663231e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476658e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596654e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794989e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525790e-11       1.451823      True  
11        0.000000e+00       0.000000     False  
12        2.785695e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898546e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293649e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548663e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210324e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177143e-12       2.130060      True  
25        0.000000e+00       0.000000     False  
26        3.503214e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 118212249/313478154 (0.3771)
Time:  93.65793093852699
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.31s/it] 50%|█████     | 5/10 [00:46<00:46,  9.31s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.27s/it] 80%|████████  | 8/10 [01:14<00:18,  9.28s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.329115           9.99          51.15
Final      10    0.501776   0.577950          80.49          98.64
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.997656     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 201358519/313478154 (0.6423)
Time:  93.61408852785826
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.93it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.32s/it] 30%|███       | 3/10 [00:27<01:05,  9.33s/it] 40%|████      | 4/10 [00:37<00:55,  9.32s/it] 50%|█████     | 5/10 [00:46<00:46,  9.33s/it] 60%|██████    | 6/10 [00:55<00:37,  9.32s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302585          10.04          49.65
Final      10    2.302675   2.302590          10.00          50.00
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.100450    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.099881    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.099569   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.099575   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.100744   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.099660   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.100046  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.099786  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.099997  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.100182  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.100006  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.099996  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.093555     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 31597941/313478154 (0.1008)
Time:  93.80750590749085
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.25it/s]100%|██████████| 1/1 [00:00<00:00,  6.24it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.31s/it] 50%|█████     | 5/10 [00:46<00:46,  9.33s/it] 60%|██████    | 6/10 [00:55<00:37,  9.33s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.303128          11.55          49.51
Final      10    0.583514   0.617367          79.62          98.56
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.896412     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.516276    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.516859    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.361070   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.360372   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.194650   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.194641   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.195350  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.067016  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.066820  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.067069  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.067190  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.066990  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.533008     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 76875004/313478154 (0.2452)
Time:  93.73766907397658
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.32s/it] 40%|████      | 4/10 [00:37<00:55,  9.32s/it] 50%|█████     | 5/10 [00:46<00:46,  9.33s/it] 60%|██████    | 6/10 [00:55<00:37,  9.32s/it] 70%|███████   | 7/10 [01:05<00:27,  9.32s/it] 80%|████████  | 8/10 [01:14<00:18,  9.33s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.34s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.321671           9.71          50.37
Final      10    0.658626   0.662399          77.32          98.19
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.861111     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.520698    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.381497    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.252387   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.207723   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.135123   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.146440   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.130804  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.082429  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.082551  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.099387  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.079112  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.080396  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.751758     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 63876719/313478154 (0.2038)
Time:  93.97504142764956
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]100%|██████████| 1/1 [00:00<00:00,  1.20it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.33s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.32s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.29s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
Train results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  7.880577e+09          10.00          50.00
Final      10  2357.140184  2.044284e+03          10.00          55.23
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.538194     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.370334    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.307210    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.223083   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.190141   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.143265   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.147013   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.124938  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.081387  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.085459  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.087292  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.085674  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.094243  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.526953     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450945e-05    8.899750e-08   0.076912    1.538147e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294464e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038073e-06    1.728672e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169522e-07    5.200793e-10   0.076228    7.789812e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561254e-07    2.033138e-10   0.075534    4.886780e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130128e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238683e-07    3.226841e-11   0.073061    2.103998e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004282e-08    1.233987e-11   0.070829    1.425630e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802335e-08    3.533979e-12   0.066115    7.976349e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819031e-08    2.921834e-12   0.066509    7.930846e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725551e-08    3.176802e-12   0.064304    7.930207e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784301e-08    3.842031e-12   0.065690    7.952877e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729265e-08    4.991512e-12   0.064391    9.028372e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451926e-05    4.614428e-09   0.074339    3.635963e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731963e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663231e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476658e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596654e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794988e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525788e-11       1.451823      True  
11        0.000000e+00       0.000000     False  
12        2.785694e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031105e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898545e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293647e-12       1.871121      True  
19        0.000000e+00       0.000000     False  
20        2.548662e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210323e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177141e-12       2.130060      True  
25        0.000000e+00       0.000000     False  
26        3.503215e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 1475791/14719818 (0.1003)
FLOP Sparsity: 55155152/313478154 (0.1759)
Time:  93.71429301891476
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]100%|██████████| 1/1 [00:00<00:00,  3.05it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.33s/it] 20%|██        | 2/10 [00:18<01:14,  9.32s/it] 30%|███       | 3/10 [00:27<01:05,  9.33s/it] 40%|████      | 4/10 [00:37<00:55,  9.33s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.33s/it] 70%|███████   | 7/10 [01:05<00:27,  9.32s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.32s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302601          10.00          49.95
Final      10    0.524286   0.584457          79.64          98.84
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.484929   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.484745   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.994922     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 143301021/313478154 (0.4571)
Time:  93.80315086897463
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.66it/s]100%|██████████| 1/1 [00:00<00:00,  6.66it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.28s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.29s/it] 50%|█████     | 5/10 [00:46<00:46,  9.30s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.27s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.27s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302585           9.87          50.00
Final      10    2.302675   2.302590          10.00          50.00
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.011574     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.009494    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.010105    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.010030   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.009840   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.010222   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.010140   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.010022  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.009963  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.009967  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.010019  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.009988  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.009982  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.010352     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 3403651/313478154 (0.0109)
Time:  93.5830621784553
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.48it/s]100%|██████████| 1/1 [00:00<00:00,  6.48it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.31s/it] 30%|███       | 3/10 [00:27<01:05,  9.29s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.30s/it] 60%|██████    | 6/10 [00:55<00:37,  9.30s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.32s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302585          10.99          49.86
Final      10    1.514943   1.443298          47.26          92.85
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.802083     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.254150    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.255263    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.107619   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.107442   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.022590   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.022703   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.022786  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.001237  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.001283  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.001272  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.001280  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.001312  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.267383     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 24470718/313478154 (0.0781)
Time:  93.7725160960108
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.27it/s]100%|██████████| 1/1 [00:00<00:00,  3.26it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.25s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:05,  9.31s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.30s/it] 70%|███████   | 7/10 [01:05<00:27,  9.30s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.30s/it]100%|██████████| 10/10 [01:33<00:00,  9.32s/it]100%|██████████| 10/10 [01:33<00:00,  9.30s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302597          10.00          50.00
Final      10    1.286692   1.213370          55.51          95.25
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.597222     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.161974    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.101888    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.049520   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.035678   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.015689   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.015796   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.012152  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.005391  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.004679  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.007243  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.007855  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.008448  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.508008     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 13936183/313478154 (0.0445)
Time:  93.76014248188585
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.29s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.29s/it] 60%|██████    | 6/10 [00:55<00:37,  9.29s/it] 70%|███████   | 7/10 [01:05<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.30s/it]100%|██████████| 10/10 [01:32<00:00,  9.29s/it]
Train results:
                train_loss      test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN       2.417717          11.73          50.17
Pre-Prune  0          NaN       2.417717          11.73          50.17
Post-Prune 0          NaN  110018.531150          10.00          50.00
Final      10    2.293576       2.304321          14.10          62.36
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.461806     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.192925    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.149197    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.089932   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.064511   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.034127   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.027095   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.013739  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.003508  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.002365  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.002667  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.003863  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.005261  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.384180     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450944e-05    8.899748e-08   0.076912    1.538146e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294464e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038072e-06    1.728672e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169522e-07    5.200793e-10   0.076228    7.789813e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561255e-07    2.033140e-10   0.075534    4.886781e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130131e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238683e-07    3.226842e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004284e-08    1.233987e-11   0.070829    1.425631e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802336e-08    3.533981e-12   0.066115    7.976351e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819032e-08    2.921837e-12   0.066509    7.930849e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176803e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784302e-08    3.842033e-12   0.065690    7.952879e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729266e-08    4.991515e-12   0.064391    9.028373e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451926e-05    4.614433e-09   0.074339    3.635964e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731963e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663231e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476658e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596655e-10       1.148655      True  
7         0.000000e+00       0.000000     False  
8         1.794989e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525789e-11       1.451824      True  
11        0.000000e+00       0.000000     False  
12        2.785695e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898545e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293649e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548664e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210324e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177144e-12       2.130060      True  
25        0.000000e+00       0.000000     False  
26        3.503219e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 18710029/313478154 (0.0597)
Time:  93.66573590785265
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.38it/s]100%|██████████| 1/1 [00:00<00:00,  3.38it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.26s/it] 20%|██        | 2/10 [00:18<01:14,  9.28s/it] 30%|███       | 3/10 [00:27<01:04,  9.28s/it] 40%|████      | 4/10 [00:37<00:55,  9.27s/it] 50%|█████     | 5/10 [00:46<00:46,  9.26s/it] 60%|██████    | 6/10 [00:55<00:37,  9.28s/it] 70%|███████   | 7/10 [01:04<00:27,  9.29s/it] 80%|████████  | 8/10 [01:14<00:18,  9.29s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]100%|██████████| 10/10 [01:32<00:00,  9.28s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302585          10.00          50.00
Final      10    2.302652   2.302590          10.00          50.00
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.988426     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.806396    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.626017    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.332425   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.052199   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.000154   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.000112   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.000000  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.975977     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 57830479/313478154 (0.1845)
Time:  93.50512155145407
Saving results.
