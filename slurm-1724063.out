Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with rand for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  6.02it/s]100%|██████████| 1/1 [00:00<00:00,  6.01it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.31s/it] 20%|██        | 2/10 [00:18<01:14,  9.32s/it] 30%|███       | 3/10 [00:27<01:05,  9.30s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.31s/it] 60%|██████    | 6/10 [00:55<00:37,  9.31s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.32s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.32s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.302582           9.66          50.48
Final      10    0.623098   0.640412          78.06          98.39
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.306713     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.314372    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.317532    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.313775   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.315433   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.316316   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.315694   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.316327  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.316288  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.316055  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.316956  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.315884  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.316285  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.306836     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance    score sum  score abs mean  \
0    -0.027050        1.005513   -46.742134        0.805873   
1     0.000000        0.000000     0.000000        0.000000   
2     0.000872        0.996963    32.139240        0.796522   
3     0.000000        0.000000     0.000000        0.000000   
4     0.002275        0.995316   167.755142        0.797035   
5     0.000000        0.000000     0.000000        0.000000   
6    -0.003706        1.001925  -546.486389        0.796965   
7     0.000000        0.000000     0.000000        0.000000   
8    -0.001777        0.997400  -524.047424        0.796505   
9     0.000000        0.000000     0.000000        0.000000   
10   -0.000449        1.003191  -264.565430        0.799367   
11    0.000000        0.000000     0.000000        0.000000   
12    0.000444        0.998147   261.771423        0.796451   
13    0.000000        0.000000     0.000000        0.000000   
14    0.000713        0.998411   841.403320        0.797582   
15    0.000000        0.000000     0.000000        0.000000   
16    0.000056        0.998994   132.429825        0.797475   
17    0.000000        0.000000     0.000000        0.000000   
18   -0.000936        1.001451 -2208.687256        0.798376   
19    0.000000        0.000000     0.000000        0.000000   
20    0.000769        1.000620  1815.271362        0.798465   
21    0.000000        0.000000     0.000000        0.000000   
22   -0.000614        1.000337 -1448.010010        0.797900   
23    0.000000        0.000000     0.000000        0.000000   
24    0.000050        1.000106   119.138557        0.797889   
25    0.000000        0.000000     0.000000        0.000000   
26   -0.034330        1.005531  -175.771698        0.800729   
27    0.000000        0.000000     0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.356814   1.392548e+03      True  
1             0.000000   0.000000e+00     False  
2             0.362516   2.936300e+04      True  
3             0.000000   0.000000e+00     False  
4             0.360056   5.876383e+04      True  
5             0.000000   0.000000e+00     False  
6             0.366784   1.175173e+05      True  
7             0.000000   0.000000e+00     False  
8             0.362983   2.348988e+05      True  
9             0.000000   0.000000e+00     False  
10            0.364203   4.714861e+05      True  
11            0.000000   0.000000e+00     False  
12            0.363814   4.697657e+05      True  
13            0.000000   0.000000e+00     False  
14            0.362275   9.408655e+05      True  
15            0.000000   0.000000e+00     False  
16            0.363027   1.881480e+06      True  
17            0.000000   0.000000e+00     False  
18            0.364049   1.883604e+06      True  
19            0.000000   0.000000e+00     False  
20            0.363073   1.883816e+06      True  
21            0.000000   0.000000e+00     False  
22            0.363694   1.882481e+06      True  
23            0.000000   0.000000e+00     False  
24            0.363478   1.882456e+06      True  
25            0.000000   0.000000e+00     False  
26            0.365542   4.099733e+03      True  
27            0.000000   0.000000e+00     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 99134651/313478154 (0.3162)
Time:  93.87950251437724
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with mag for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]100%|██████████| 1/1 [00:00<00:00,  5.92it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.32s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.32s/it] 40%|████      | 4/10 [00:37<00:55,  9.33s/it] 50%|█████     | 5/10 [00:46<00:46,  9.32s/it] 60%|██████    | 6/10 [00:55<00:37,  9.33s/it] 70%|███████   | 7/10 [01:05<00:27,  9.33s/it] 80%|████████  | 8/10 [01:14<00:18,  9.33s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.329496           8.82          49.56
Final      10    0.519265   0.608344          79.73          98.65
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.938657     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.701606    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.699517    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.588182   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.587646   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.442337   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.441876   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.442021  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.276644  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.276784  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.277225  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.277301  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.277092  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.720117     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

    score mean  score variance     score sum  score abs mean  \
0     0.216417        0.026268    373.968018        0.216417   
1     0.000000        0.000000      0.000000        0.000000   
2     0.046945        0.001257   1730.575928        0.046945   
3     0.000000        0.000000      0.000000        0.000000   
4     0.046975        0.001261   3463.390137        0.046975   
5     0.000000        0.000000      0.000000        0.000000   
6     0.033302        0.000633   4910.584961        0.033302   
7     0.000000        0.000000      0.000000        0.000000   
8     0.033267        0.000631   9810.975586        0.033267   
9     0.000000        0.000000      0.000000        0.000000   
10    0.023493        0.000314  13856.869141        0.023493   
11    0.000000        0.000000      0.000000        0.000000   
12    0.023487        0.000315  13853.344727        0.023487   
13    0.000000        0.000000      0.000000        0.000000   
14    0.023506        0.000315  27728.505859        0.023506   
15    0.000000        0.000000      0.000000        0.000000   
16    0.016622        0.000158  39216.730469        0.016622   
17    0.000000        0.000000      0.000000        0.000000   
18    0.016615        0.000157  39200.449219        0.016615   
19    0.000000        0.000000      0.000000        0.000000   
20    0.016630        0.000158  39235.941406        0.016630   
21    0.000000        0.000000      0.000000        0.000000   
22    0.016630        0.000158  39234.457031        0.016630   
23    0.000000        0.000000      0.000000        0.000000   
24    0.016624        0.000158  39220.398438        0.016624   
25    0.000000        0.000000      0.000000        0.000000   
26    0.049012        0.001357    250.940384        0.049012   
27    0.000000        0.000000      0.000000        0.000000   

    score abs variance  score abs sum  prunable  
0             0.026268     373.968018      True  
1             0.000000       0.000000     False  
2             0.001257    1730.575928      True  
3             0.000000       0.000000     False  
4             0.001261    3463.390137      True  
5             0.000000       0.000000     False  
6             0.000633    4910.584961      True  
7             0.000000       0.000000     False  
8             0.000631    9810.975586      True  
9             0.000000       0.000000     False  
10            0.000314   13856.869141      True  
11            0.000000       0.000000     False  
12            0.000315   13853.344727      True  
13            0.000000       0.000000     False  
14            0.000315   27728.505859      True  
15            0.000000       0.000000     False  
16            0.000158   39216.730469      True  
17            0.000000       0.000000     False  
18            0.000157   39200.449219      True  
19            0.000000       0.000000     False  
20            0.000158   39235.941406      True  
21            0.000000       0.000000     False  
22            0.000158   39234.457031      True  
23            0.000000       0.000000     False  
24            0.000158   39220.398438      True  
25            0.000000       0.000000     False  
26            0.001357     250.940384      True  
27            0.000000       0.000000     False  
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 145383527/313478154 (0.4638)
Time:  94.00738693680614
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with snip for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.08it/s]100%|██████████| 1/1 [00:00<00:00,  3.07it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.29s/it] 30%|███       | 3/10 [00:27<01:05,  9.32s/it] 40%|████      | 4/10 [00:37<00:55,  9.30s/it] 50%|█████     | 5/10 [00:46<00:46,  9.30s/it] 60%|██████    | 6/10 [00:55<00:37,  9.32s/it] 70%|███████   | 7/10 [01:05<00:27,  9.31s/it] 80%|████████  | 8/10 [01:14<00:18,  9.30s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.31s/it]
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.441820           8.77          50.62
Final      10    0.585948   0.627286          78.30          98.33
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.961227     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.810818    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.696221    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.557285   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.497189   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.393904   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.428480   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.405860  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.308576  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.317776  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.319197  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.241647  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.242628  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.894727     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   2.148630e-06    8.224373e-12   0.003713    2.148630e-06   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   4.373166e-07    4.509947e-13   0.016121    4.373166e-07   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   3.023458e-07    3.032996e-13   0.022291    3.023458e-07   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   1.801422e-07    1.370182e-13   0.026563    1.801422e-07   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   1.432513e-07    8.960762e-14   0.042247    1.432513e-07   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  8.976853e-08    3.945879e-14   0.052948    8.976853e-08   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  9.580083e-08    4.068017e-14   0.056506    9.580083e-08   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  8.543239e-08    3.210806e-14   0.100780    8.543239e-08   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  5.670629e-08    1.667912e-14   0.133787    5.670629e-08   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  5.675454e-08    1.528053e-14   0.133901    5.675454e-08   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  6.345071e-08    2.087551e-14   0.149699    6.345071e-08   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  5.213338e-08    2.200064e-14   0.122998    5.213338e-08   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  5.339874e-08    2.426873e-14   0.125983    5.339874e-08   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  2.434253e-06    1.646455e-11   0.012463    2.434253e-06   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         8.224373e-12       0.003713      True  
1         0.000000e+00       0.000000     False  
2         4.509947e-13       0.016121      True  
3         0.000000e+00       0.000000     False  
4         3.032996e-13       0.022291      True  
5         0.000000e+00       0.000000     False  
6         1.370182e-13       0.026563      True  
7         0.000000e+00       0.000000     False  
8         8.960762e-14       0.042247      True  
9         0.000000e+00       0.000000     False  
10        3.945879e-14       0.052948      True  
11        0.000000e+00       0.000000     False  
12        4.068017e-14       0.056506      True  
13        0.000000e+00       0.000000     False  
14        3.210806e-14       0.100780      True  
15        0.000000e+00       0.000000     False  
16        1.667912e-14       0.133787      True  
17        0.000000e+00       0.000000     False  
18        1.528053e-14       0.133901      True  
19        0.000000e+00       0.000000     False  
20        2.087551e-14       0.149699      True  
21        0.000000e+00       0.000000     False  
22        2.200064e-14       0.122998      True  
23        0.000000e+00       0.000000     False  
24        2.426873e-14       0.125983      True  
25        0.000000e+00       0.000000     False  
26        1.646455e-11       0.012463      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 4657709/14719818 (0.3164)
FLOP Sparsity: 146081773/313478154 (0.4660)
Time:  93.80387093126774
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with grasp for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.33s/it] 20%|██        | 2/10 [00:18<01:14,  9.30s/it] 30%|███       | 3/10 [00:27<01:05,  9.33s/it] 40%|████      | 4/10 [00:37<00:55,  9.31s/it] 50%|█████     | 5/10 [00:46<00:46,  9.31s/it] 60%|██████    | 6/10 [00:55<00:37,  9.33s/it] 70%|███████   | 7/10 [01:05<00:28,  9.35s/it] 80%|████████  | 8/10 [01:14<00:18,  9.35s/it] 90%|█████████ | 9/10 [01:24<00:09,  9.35s/it]100%|██████████| 10/10 [01:33<00:00,  9.34s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]
Train results:
                train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN  2.417717e+00          11.73          50.17
Pre-Prune  0          NaN  2.417717e+00          11.73          50.17
Post-Prune 0          NaN  4.058672e+10          10.00          50.00
Final      10    2.081715  2.000815e+00          27.19          78.11
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.560185     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.483209    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.455471    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.407023   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.384284   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.349433   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.371523   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.364050  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.328551  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.346779  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.311499  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.260490  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.265916  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.568750     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance  score sum  score abs mean  \
0   4.450944e-05    8.899747e-08   0.076912    1.538146e-04   
1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
2   2.075165e-06    4.294465e-09   0.076499    2.520991e-05   
3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
4   1.038072e-06    1.728672e-09   0.076535    1.590886e-05   
5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
6   5.169522e-07    5.200793e-10   0.076228    7.789811e-06   
7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
8   2.561254e-07    2.033138e-10   0.075534    4.886780e-06   
9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
10  1.238559e-07    5.130129e-11   0.073053    2.461452e-06   
11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
12  1.238684e-07    3.226842e-11   0.073061    2.103999e-06   
13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
14  6.004284e-08    1.233988e-11   0.070829    1.425630e-06   
15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
16  2.802335e-08    3.533981e-12   0.066115    7.976350e-07   
17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
18  2.819031e-08    2.921836e-12   0.066509    7.930848e-07   
19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
20  2.725552e-08    3.176802e-12   0.064304    7.930209e-07   
21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
22  2.784301e-08    3.842032e-12   0.065690    7.952877e-07   
23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
24  2.729265e-08    4.991513e-12   0.064391    9.028373e-07   
25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   
26  1.451926e-05    4.614427e-09   0.074339    3.635963e-05   
27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   

    score abs variance  score abs sum  prunable  
0         6.731962e-08       0.265792      True  
1         0.000000e+00       0.000000     False  
2         3.663231e-09       0.929338      True  
3         0.000000e+00       0.000000     False  
4         1.476658e-09       1.172928      True  
5         0.000000e+00       0.000000     False  
6         4.596653e-10       1.148654      True  
7         0.000000e+00       0.000000     False  
8         1.794988e-10       1.441170      True  
9         0.000000e+00       0.000000     False  
10        4.525788e-11       1.451823      True  
11        0.000000e+00       0.000000     False  
12        2.785695e-11       1.240989      True  
13        0.000000e+00       0.000000     False  
14        1.031106e-11       1.681742      True  
15        0.000000e+00       0.000000     False  
16        2.898545e-12       1.881857      True  
17        0.000000e+00       0.000000     False  
18        2.293648e-12       1.871122      True  
19        0.000000e+00       0.000000     False  
20        2.548663e-12       1.870971      True  
21        0.000000e+00       0.000000     False  
22        3.210324e-12       1.876319      True  
23        0.000000e+00       0.000000     False  
24        4.177143e-12       2.130060      True  
25        0.000000e+00       0.000000     False  
26        3.503214e-09       0.186161      True  
27        0.000000e+00       0.000000     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 118212297/313478154 (0.3771)
Time:  94.07421761285514
Saving results.
Loading cifar10 dataset.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Creating lottery-vgg16 model.
Pre-Train for 0 epochs.
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/vikashm/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
0it [00:00, ?it/s]0it [00:00, ?it/s]
Pruning with synflow for 1 epochs.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]100%|██████████| 1/1 [00:00<00:00,  3.06it/s]Post-Training for 10 epochs.

  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:09<01:23,  9.30s/it] 20%|██        | 2/10 [00:18<01:14,  9.31s/it] 30%|███       | 3/10 [00:27<01:05,  9.32s/it] 40%|████      | 4/10 [00:37<00:56,  9.34s/it] 50%|█████     | 5/10 [00:46<00:46,  9.35s/it] 60%|██████    | 6/10 [00:56<00:37,  9.35s/it] 70%|███████   | 7/10 [01:05<00:28,  9.34s/it] 80%|████████  | 8/10 [01:14<00:18,  9.31s/it] 90%|█████████ | 9/10 [01:23<00:09,  9.31s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]100%|██████████| 10/10 [01:33<00:00,  9.33s/it]
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:205: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/vikashm/.local/lib/python3.6/site-packages/numpy/core/_methods.py:216: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims)
Train results:
                train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0          NaN   2.417717          11.73          50.17
Pre-Prune  0          NaN   2.417717          11.73          50.17
Post-Prune 0          NaN   2.329115           9.99          51.15
Final      10     0.48098   0.582650          80.53          98.67
Prune results:
             module   param  sparsity     size             shape     flops  \
0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)   1769472   
1    layers.0.conv    bias  1.000000       64             (64,)     65536   
2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  37748736   
3    layers.1.conv    bias  1.000000       64             (64,)     65536   
4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  18874368   
5    layers.3.conv    bias  1.000000      128            (128,)     32768   
6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  37748736   
7    layers.4.conv    bias  1.000000      128            (128,)     32768   
8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  18874368   
9    layers.6.conv    bias  1.000000      256            (256,)     16384   
10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  37748736   
11   layers.7.conv    bias  1.000000      256            (256,)     16384   
12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  37748736   
13   layers.8.conv    bias  1.000000      256            (256,)     16384   
14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  18874368   
15  layers.10.conv    bias  1.000000      512            (512,)      8192   
16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  37748736   
17  layers.11.conv    bias  1.000000      512            (512,)      8192   
18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  37748736   
19  layers.12.conv    bias  1.000000      512            (512,)      8192   
20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)   9437184   
21  layers.14.conv    bias  1.000000      512            (512,)      2048   
22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)   9437184   
23  layers.15.conv    bias  1.000000      512            (512,)      2048   
24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)   9437184   
25  layers.16.conv    bias  1.000000      512            (512,)      2048   
26              fc  weight  0.997656     5120         (10, 512)      5120   
27              fc    bias  1.000000       10             (10,)        10   

      score mean  score variance     score sum  score abs mean  \
0   1.704298e+19             inf  2.945027e+22    1.704298e+19   
1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
2   7.988897e+17             inf  2.945027e+22    7.988897e+17   
3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
4   3.994448e+17             inf  2.945027e+22    3.994448e+17   
5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
6   1.997224e+17             inf  2.945027e+22    1.997224e+17   
7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
8   9.986122e+16             inf  2.945027e+22    9.986122e+16   
9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
10  4.993060e+16             inf  2.945027e+22    4.993060e+16   
11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
12  4.993061e+16             inf  2.945027e+22    4.993061e+16   
13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
14  2.496530e+16             inf  2.945027e+22    2.496530e+16   
15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   
17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   
19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
20  1.248265e+16             inf  2.945026e+22    1.248265e+16   
21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
22  1.248265e+16             inf  2.945026e+22    1.248265e+16   
23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
24  1.248265e+16             inf  2.945028e+22    1.248265e+16   
25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   
26  5.752006e+18             inf  2.945027e+22    5.752006e+18   
27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   

    score abs variance  score abs sum  prunable  
0                  inf   2.945027e+22      True  
1         0.000000e+00   0.000000e+00     False  
2                  inf   2.945027e+22      True  
3         0.000000e+00   0.000000e+00     False  
4                  inf   2.945027e+22      True  
5         0.000000e+00   0.000000e+00     False  
6                  inf   2.945027e+22      True  
7         0.000000e+00   0.000000e+00     False  
8                  inf   2.945027e+22      True  
9         0.000000e+00   0.000000e+00     False  
10                 inf   2.945027e+22      True  
11        0.000000e+00   0.000000e+00     False  
12                 inf   2.945027e+22      True  
13        0.000000e+00   0.000000e+00     False  
14                 inf   2.945027e+22      True  
15        0.000000e+00   0.000000e+00     False  
16        9.402646e+31   2.945026e+22      True  
17        0.000000e+00   0.000000e+00     False  
18        9.415211e+31   2.945027e+22      True  
19        0.000000e+00   0.000000e+00     False  
20                 inf   2.945026e+22      True  
21        0.000000e+00   0.000000e+00     False  
22                 inf   2.945026e+22      True  
23        0.000000e+00   0.000000e+00     False  
24                 inf   2.945028e+22      True  
25        0.000000e+00   0.000000e+00     False  
26                 inf   2.945027e+22      True  
27        0.000000e+00   0.000000e+00     False  
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 201358519/313478154 (0.6423)
Time:  93.99327272828668
Saving results.
